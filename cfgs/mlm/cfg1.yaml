##############################################
# Masked Language Modeling
##############################################

# Paths
paths:
  data:
    train:
      # - pii-label-specific-data/all_labels_mdd.json
      - mdd-pii-detection-removal-from-educational-data/train_f2.json
      # - mdd-gen/mdd_Mistral-7x8B-Instruct-v0.1_3.2K.json
      - mdd-gen/MixtralQ5_placeholder_pii_mimic_training_3.6K_v9.json
      # - mdd-gen/mistral_placeholder_mimic_2.8K_v0.json
      # - vw-output/moredata_dataset_fixed.json
      - vw-output/pii_dataset_fixed.json
      - mixtral-original-prompt/Fake_data_1850_218.json
      - pii-dd-mistral-generated/mixtral-8x7b-v1.json
      - pii-mixtral8x7b-generated-essays/mpware_mixtral8x7b_v1.1-no-i-username.json
      # - pii-detect-gpt3-5-synthetic-data-8k/mdd_PII_Detect_GPT3.5_Generated_data_v1.json
    val: mdd-pii-detection-removal-from-educational-data/val_f2.json

# Seed
seed: 42

# Block size and MLM Prob.
block_size: 1024
mlm_probability: 0.20

# Model and tokenizer
model:
  name: deberta-v3-large
  freeze:
    apply: False
    num_layers: 8

# Trainer arguments
train_args:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 3
  learning_rate: 5.0E-5
  fp16: True
  gradient_checkpointing: True
  use_reentrant: False
  eval_epoch_fraction: 0.2
  warmup_ratio: 0.05
  weight_decay: 0.0
  metric_for_best_model: f1
  greater_is_better: True
  lr_scheduler_type: 'linear'

# Tokenizer
tokenizer:
  add_tokens: Null
  do_lower: False
  use_fast: True
  pad_to_multiple_of: 16
  max_token_length: 1024
